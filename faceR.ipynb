{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3d92fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "0d3d92fe",
        "outputId": "c5d7bcdd-414d-4a35-f1d0-d00f488ce5db"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-0755b4c49b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspeech_recognition\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'drowsinessDetection.tflite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, model_content, experimental_delegates, num_threads, experimental_op_resolver_type, experimental_preserve_all_tensors)\u001b[0m\n\u001b[1;32m    455\u001b[0m           _interpreter_wrapper.CreateWrapperFromFile(\n\u001b[1;32m    456\u001b[0m               \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_resolver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_op_registerers_by_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m               custom_op_registerers_by_func, experimental_preserve_all_tensors))\n\u001b[0m\u001b[1;32m    458\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Failed to open {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Could not open 'drowsinessDetection.tflite'."
          ]
        }
      ],
      "source": [
        "# https://github.com/tensorflow/examples/blob/master/lite/examples/image_classification/raspberry_pi/classify_picamera.py\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import pygame\n",
        "import time\n",
        "from gtts import gTTS\n",
        "import random\n",
        "from IPython.display import Audio\n",
        "import speech_recognition as sr\n",
        "\n",
        "interpreter = tf.lite.Interpreter(model_path='drowsinessDetection.tflite')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gtts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COjOz1F6WDQB",
        "outputId": "9860712b-cf5f-4eef-876a-78d9d1b621b7"
      },
      "id": "COjOz1F6WDQB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gtts\n",
            "  Downloading gTTS-2.2.4-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gtts) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gtts) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from gtts) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gtts) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gtts) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gtts) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gtts) (3.0.4)\n",
            "Installing collected packages: gtts\n",
            "Successfully installed gtts-2.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install speech_recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3fIrIhqIocd",
        "outputId": "1ef90a7f-e64e-4fa9-8fc5-b793c588146c"
      },
      "id": "y3fIrIhqIocd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement speech_recognition (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for speech_recognition\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50bb283f",
      "metadata": {
        "id": "50bb283f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "0596e46f-b962-4c16-82c4-c3f8ff878ab1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-bd52b81b624d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmusic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'004_b.mp3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mface_cascade_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'haarcascade_frontalface_alt.xml'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0meyes_cascade_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'haarcascade_eye_tree_eyeglasses.xml'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pygame' is not defined"
          ]
        }
      ],
      "source": [
        "pygame.mixer.init()\n",
        "pygame.mixer.music.load('004_b.mp3')\n",
        "\n",
        "face_cascade_name = 'haarcascade_frontalface_alt.xml'\n",
        "eyes_cascade_name = 'haarcascade_eye_tree_eyeglasses.xml'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a0b0fbc",
      "metadata": {
        "id": "1a0b0fbc"
      },
      "outputs": [],
      "source": [
        "face_cascade = cv2.CascadeClassifier()\n",
        "eyes_cascade = cv2.CascadeClassifier()\n",
        "\n",
        "#-- 1. Load the cascades\n",
        "if not face_cascade.load(cv2.samples.findFile(face_cascade_name)):\n",
        "    print('--(!)Error loading face cascade')\n",
        "    exit(0)\n",
        "if not eyes_cascade.load(cv2.samples.findFile(eyes_cascade_name)):\n",
        "    print('--(!)Error loading eyes cascade')\n",
        "    exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d63754d",
      "metadata": {
        "id": "4d63754d"
      },
      "outputs": [],
      "source": [
        "#-- 2. Load TF model\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "SZ = 24\n",
        "status = 'Awake'\n",
        "number_closed = 0\n",
        "closed_limit = 7\n",
        "show_frame = None\n",
        "sign = None\n",
        "color = None\n",
        "\n",
        "frame_width = 640\n",
        "frame_height = 480\n",
        "frame_resolution = [frame_width, frame_height]\n",
        "frame_rate = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09219fa9",
      "metadata": {
        "id": "09219fa9"
      },
      "outputs": [],
      "source": [
        "def set_input_tensor(interpreter, image):\n",
        "  tensor_index = interpreter.get_input_details()[0]['index']\n",
        "  input_tensor = interpreter.tensor(tensor_index)()[0]\n",
        "  input_tensor[:, :] = image\n",
        "\n",
        "\n",
        "def classify_image(interpreter, image, top_k=1):\n",
        "  set_input_tensor(interpreter, image)\n",
        "  interpreter.invoke()\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "  output = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
        "\n",
        "  # If the model is quantized (uint8 data), then dequantize the results\n",
        "  if output_details['dtype'] == np.uint8:\n",
        "    scale, zero_point = output_details['quantization']\n",
        "    output = scale * (output - zero_point)\n",
        "\n",
        "  ordered = np.argpartition(-output, top_k)\n",
        "  return [(i, output[i]) for i in ordered[:top_k]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc9e5f06",
      "metadata": {
        "id": "bc9e5f06"
      },
      "outputs": [],
      "source": [
        "def quiz():\n",
        "    f = open(\"quiz.csv\")\n",
        "    lines = f.readlines()\n",
        "    lines = lines[1:]\n",
        "    random.shuffle(lines)\n",
        "\n",
        "    r = sr.Recognizer()\n",
        "    mic = sr.Microphone()\n",
        "    \n",
        "    for line in lines:\n",
        "        quiz, answer = line.rstrip().split(',')\n",
        "        correct = None\n",
        "        while not correct:\n",
        "            if correct is None:\n",
        "                kor_wav = gTTS(quiz, lang = 'ko')\n",
        "                kor_wav.save(\"kor.wav\")\n",
        "                display(Audio('kor.wav', autoplay=True))\n",
        "            else:\n",
        "                kor_wav2 = gTTS(\"다시 말씀해주세요\", lang='ko')\n",
        "                kor_wav2.save(\"kor2.wav\")\n",
        "                display(Audio('kor2.wav', autoplay=True))\n",
        "\n",
        "            ans = None\n",
        "\n",
        "            while not ans:\n",
        "                with mic as source :\n",
        "                    audio = r.listen(source)\n",
        "                    time.sleep(1)\n",
        "                    ans = r.recognize_google(audio, language = \"ko-KR\")\n",
        "                    time.sleep(1)\n",
        "\n",
        "            print(ans)\n",
        "            if ans == \"멈춰\":\n",
        "                break\n",
        "            if answer == ans:\n",
        "\n",
        "                correct = True\n",
        "                kor_wav3 = gTTS(\"정답입니다!\", lang='ko')\n",
        "                kor_wav3.save(\"kor3.wav\")\n",
        "                display(Audio('kor3.wav', autoplay=True))\n",
        "            else:\n",
        "                correct = False\n",
        "                kor_wav4 = gTTS(f\"정답은 {answer}입니다.\", lang='ko')\n",
        "                kor_wav4.save(\"kor4.wav\")\n",
        "                display(Audio('kor4.wav', autoplay=True))\n",
        "            time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d454317",
      "metadata": {
        "id": "5d454317",
        "outputId": "19ad5324-3130-4aa6-c0f0-fcc67e75b463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== A frame took 13.763 seconds\n"
          ]
        }
      ],
      "source": [
        "# initialize the camera and grab a reference to the raw camera capture\n",
        "capture = cv2.VideoCapture(0)\n",
        "capture.set(cv2.CAP_PROP_FRAME_WIDTH, frame_width)\n",
        "capture.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_height)\n",
        "fps = capture.get(cv2.CAP_PROP_FPS)\n",
        "# allow the camera to warmup\n",
        "time.sleep(0.1)\n",
        "word = None\n",
        "r = sr.Recognizer()\n",
        "mic = sr.Microphone()\n",
        "# capture frames from the camera\n",
        "\n",
        "while capture.isOpened():\n",
        "    ret, frame = capture.read()\n",
        "    start_time = time.time()\n",
        "    image = cv2.GaussianBlur(frame,(5,5), cv2.BORDER_DEFAULT)\n",
        "    show_frame = image\n",
        "    # transform gray image\n",
        "    height,width = image.shape[:2]\n",
        "    frame_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    frame_gray = cv2.equalizeHist(frame_gray)\n",
        "    #-- Detect faces\n",
        "    faces = face_cascade.detectMultiScale(frame_gray)\n",
        "    with mic as source :\n",
        "        audio = r.listen(source)\n",
        "    try:\n",
        "        word = r.recognize_google(audio, language = \"ko-KR\")\n",
        "    except:\n",
        "        word = None\n",
        "    if word == \"퀴즈\":\n",
        "        quiz()\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "        \n",
        "\n",
        "    for (x,y,w,h) in faces:\n",
        "        show_frame = cv2.rectangle(show_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "        faceROI = frame_gray[y:y+h,x:x+w]\n",
        "        #-- In each face, detect eyes\n",
        "        eyes = eyes_cascade.detectMultiScale(faceROI)\n",
        "        results = []\n",
        "\n",
        "        for (x2,y2,w2,h2) in eyes:\n",
        "            eye_center = (x + x2 + w2//2, y + y2 + h2//2)\n",
        "            radius = int(round((w2 + h2)*0.25))\n",
        "            show_frame = cv2.circle(show_frame, eye_center, radius, (0, 255, 255 ), 2)\n",
        "            eye = faceROI[y2:y2+h2,x2:x2+w2]\n",
        "            eye = cv2.resize(eye,(SZ,SZ))\n",
        "            eye= eye/255\n",
        "            eye=  eye.reshape(SZ,SZ,-1)\n",
        "            eye = np.expand_dims(eye,axis=0)\n",
        "            #result = model.predict_classes(eye)\n",
        "            prediction = classify_image(interpreter, eye)\n",
        "            result, confidence = prediction[0]\n",
        "            results.append(result)\n",
        "        print(results, np.mean(results))\n",
        "        if( np.mean(results)==1 ):\n",
        "            color = (0, 255, 0)\n",
        "            status = 'Awake'\n",
        "            number_closed = number_closed - 1\n",
        "            if( number_closed<0 ):\n",
        "                number_closed = 0\n",
        "        else:\n",
        "            color = (0, 0, 255)\n",
        "            status = 'Sleep'\n",
        "            number_closed = number_closed + 1\n",
        "            \n",
        "        sign = status + ', Sleep count : ' + str(number_closed) + ' / ' + str(closed_limit)\n",
        "        if( number_closed > closed_limit ):\n",
        "            show_frame = frame_gray\n",
        "            # play SOUND\n",
        "            if(pygame.mixer.music.get_busy()==False):\n",
        "                pygame.mixer.music.play()\n",
        "            #while pygame.mixer.music.get_busy() == True:\n",
        "            #    continue\n",
        "        \n",
        "    # show the frame\n",
        "    cv2.putText(show_frame, sign , (10,height-20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "    cv2.imshow(\"Drowsiness Detection\", show_frame)\n",
        "    end_time = time.time()\n",
        "    process_time = end_time - start_time\n",
        "    print(\"=== A frame took {:.3f} seconds\".format(process_time))\n",
        "    # if the `q` key was pressed, break from the loop\n",
        "    ret, frame = capture.read()\n",
        "    \n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "    if key == ord(\"q\"):\n",
        "        break\n",
        "    ret, frame = capture.read()\n",
        "    if (type(frame) == type(None)):\n",
        "        break\n",
        "\n",
        "    \n",
        "    if cv2.waitKey(1)& 0xFF == ord('q'):\n",
        "        break\n",
        "capture.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d96690",
      "metadata": {
        "id": "f0d96690"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b837731a",
      "metadata": {
        "id": "b837731a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18da091c",
      "metadata": {
        "id": "18da091c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}